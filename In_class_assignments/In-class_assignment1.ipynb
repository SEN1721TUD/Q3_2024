{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEN1721 Travel Behaviour Research\n",
    "\n",
    "## `In-class assignment 1:`\n",
    "## `Introduction to Latent classs models`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q3 2025**<br>\n",
    "**Instructor:** Sander van Cranenburgh<br>\n",
    "**TA:**  Gabriel Nova <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**In-class assignments aim to:**<br>\n",
    "* Illustrate how models and theory discussed in the classroom work out in practice.\n",
    "* Help you gather hands-on modelling and data analysis skills.\n",
    "\n",
    "**In-class assignments are:**<br>\n",
    "* Learning environments where you work with Python and get support from TA and fellow students.\n",
    "* Not graded and do not have to be submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Use of AI tools`\n",
    "AI tools, such as ChatGPT and Copilot, are great tools to assist with programming. Moreover, in your later careers, you will work in a world where such tools are widely available. As such, we **encourage** you to use AI tools **effectively**. However, be careful not to overestimate the capacity of AI tools! AI tools cannot replace you: you still have to conceptualise the problem, dissect it and structure it to conduct proper analysis. We recommend being especially **reticent** with using AI tools for the more conceptual and reflection-oriented questions. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Learning objectives In-class assignment 1`**\n",
    "\n",
    "After completing the in-class assignment, you will be able to:\n",
    "1. Discover choice data\n",
    "1. Estimate RUM-based multinomial logit discrete choice models and interpret the results\n",
    "1. Estimate Latent Class choice models and interpret the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Import packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will import all the Python libraries that we will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biogeme\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme.expressions import Beta, Variable,PanelLikelihoodTrajectory\n",
    "\n",
    "# Import custom estimation functions for Biogeme\n",
    "from bio_estimation_fcns import estimate_mnl, estimate_LC, print_results\n",
    "\n",
    "# General python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# Random number generator\n",
    "from random import random as rand\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Pandas setting to show all columns when displaying a pandas dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1. Load and explore the data set` <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Load the data set`** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create that path to the data file\n",
    "data_path =  Path(f'data/Route_choice_data.csv')\n",
    "\n",
    "# Load the data as a pandas dataframe\n",
    "df = pd.read_csv(data_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of variables**<br>\n",
    "\n",
    "The data contain 3,510 stated route choices from 388 respondent. Each alternative is defined by four attributes: TT, CONG, VAR, and TC.\n",
    "Below is a description of the variables in the data set. For more information about the data, you can have a look at the original paper by [Chorus et al. (2012)](https://link.springer.com/article/10.1007/s11116-012-9444-3). \n",
    "\n",
    "\n",
    "| Variable | Description                                                       | Data type                 |\n",
    "|----------|-------------------------------------------------------------------|---------------------------|\n",
    "| Survey   | Identifier for the survey                                         | Integer                   |\n",
    "| ID       | Unique identifier for each respondent                             | Integer                   |\n",
    "| Quest    | Order of the choice tasks per respondent                          | Integer                   |\n",
    "| CHOICE   | Chosen alternative                                                | Integer                   |\n",
    "| TT1      | Travel Time for Alternative 1                                     | Integer                   |\n",
    "| CONG1    | Percentage of travel time in congestion for Alternative 1         | Integer                   |\n",
    "| VAR1     | Travel time variability for Alternative 1                         | Integer                   |\n",
    "| TC1      | Travel Cost for Alternative 1                                     | Float                     |\n",
    "| TT2      | Travel Time for Alternative 2                                     | Integer                   |\n",
    "| CONG2    | Percentage of travel time in congestion for Alternative 2         | Integer                   |\n",
    "| VAR2     | Travel time variability for Alternative 2                         | Integer                   |\n",
    "| TC2      | Travel Cost for Alternative 2                                     | Float                     |\n",
    "| TT3      | Travel Time for Alternative 3                                     | Integer                   |\n",
    "| CONG3    | Percentage of travel time in congestion for Alternative 3         | Integer                   |\n",
    "| VAR3     | Travel time variability for Alternative 3                         | Integer                   |\n",
    "| TC3      | Travel Cost for Alternative 3                                     | Float                     |\n",
    "| AV1      | Availability of Alternative 1 (0: not available, 1: available)    | Integer                   |\n",
    "| AV2      | Availability of Alternative 2 (0: not available, 1: available)    | Integer                   |\n",
    "| AV3      | Availability of Alternative 3 (0: not available, 1: available)    | Integer                   |\n",
    "| age      | Age of the respondent                                             | Integer                   |\n",
    "| edu      | Educational level                                                 | Integer                   |\n",
    "| edufin   | Max education level                                               | Integer                   |\n",
    "| sex      | Gender of the respondent (e.g., male, female)                     | Integer                   |\n",
    "| E1       | I considered it difficult to make choices across the presented alternatives in this choice experiment | Integer                   |\n",
    "| E2       | I considered it important to make the ‘right’ choice across the presented alternatives                | Integer                   |\n",
    "| E3       | When considering a new job or residential location, having a pleasant commute takes a central place in my decision | Integer      |\n",
    "| E4       | In general, I think making choices is difficult                                                       | Integer                   |\n",
    "| E5       | When decisions are important I consider making choices difficult                                      | Integer                   |\n",
    "| E6       | When making choices, I consider the possibility that I will regret my choice in hindsight             | Integer                   |\n",
    "| E7       | When making choices, I prefer a choice set that is as large as possible                               | Integer                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data, it is helpful to carefully look at the choice tasks that were presented to the respondents. Below, you see a screenshot of one of the choice tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![State choice preference](data/Picture1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Explore the data set`**<br>\n",
    "\n",
    "Now, let's explore the data set and examine the variables in the data.<br>\n",
    "You can use `head()` to look at the first 5 rows of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Descriptive statistics`**<br>\n",
    "\n",
    "We can use `describe()` to view descriptive statistics, such as count, mean, std, min, percentiles, and max about the **attribute levels** of the alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**<br>\n",
    "\n",
    "        --> As can be seen, the data look fine: it does not contain outliers of NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of choice task per individual\n",
    "print(f\"There are {df['ID'].nunique()} individuals in the dataset\")\n",
    "print(f\"The number of choice tasks per individual is:\")\n",
    "print(df['ID'].value_counts().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**<br>\n",
    "\n",
    "        --> Almost all respondents have answered all 9 stated choice questions. However, for some reason, there are 2 respondents who have answered 18 questions. Perhaps, they participated in the two survey arms. Below, we remove these two respondents from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID with > 9 choice tasks\n",
    "df = df[df['ID'].map(df['ID'].value_counts()) <= 9]\n",
    "\n",
    "# Check the number of choice task per individual\n",
    "print(f\"There are {df['ID'].nunique()} individuals in the dataset\")\n",
    "print(f\"The number of choice tasks per individual is:\")\n",
    "print(df['ID'].value_counts().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Correlations`**<br>\n",
    "\n",
    "It is useful to check the correlation between the variables in your data set. We can use the `corr()` function to compute the correlation matrix, and show the correlation matrix using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlations\n",
    "fig, axes = plt.subplots(figsize=(8, 8))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = df[['age', 'edu', 'edufin', 'sex','E1','E2','E3','E4' ,'E5','E6','E7']].corr()\n",
    "\n",
    "# Create upper triangular matrix to mask the upper triangular part of the heatmap\n",
    "corr_mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap (because it looks better)\n",
    "corr_cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(corr.round(2), mask=corr_mask, cmap=corr_cmap,\n",
    "            annot=True, square=True, linewidths=.5, ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**<br>\n",
    "\n",
    "        --> As can be seen, the data look fine. The correlation matrix shows that none of the variables highly correlate. This is good because it means none of the variables are not redundant. The highest correllation is between E4 and E5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Frequency and percentage of choices`**<br>\n",
    "\n",
    "When modelling choices, we are also interested in the frequency at which alternatives are chosen. In experiments with **unlabelled** alternatives (like this one), this analysis tells us whether the choices are 'balanced'. This means that the alternatives have been chosen in a similar proportion. If the data are not balanced, it may indicate that the experimental design was insufficiently randomised. Alternatively, it may suggest that respondents not seriously answered the questions, as they only clicked the \"right\" or \"left\" alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of times each  alternative is chosen\n",
    "choice_freq = df['CHOICE'].value_counts()\n",
    "\n",
    "# Calculate the percentage of the chosen alternatives\n",
    "choice_percent = round(choice_freq / len(df['CHOICE']) * 100,2)\n",
    "\n",
    "# Table Summary\n",
    "choice_table = pd.DataFrame({'Choice': choice_freq.index, 'Frequency': choice_freq.values, 'Percentage':choice_percent.values} )\n",
    "\n",
    "# Show the table\n",
    "choice_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**<br>\n",
    "\n",
    "        --> As can be seen, all alternatives attain an (almost) equal share (although Alt 1 is somewhat overrepresented). This is what we would expect in a well-designed experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2. Creating Biogeme database and variables` <br>\n",
    "Now that we have developed a feeling for our data and prepocessed them, we can start with estimating discrete choice models. For this, we will use the Python package called `Biogeme`. The first thing we need to do is to create a Biogeme database. This database will contain all the variables that we will use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Biogeme database`**<br>\n",
    "To use Biogeme for estimation, we first need to create the data set as a Biogeme database object using `db.Database()`. This object contains the data in a format compatible with the library functions for model estimation in Biogeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.Database takes two arguments:\n",
    "# (1) a name (string) \n",
    "# (2) a data set (pandas dataframe)\n",
    "biodata = db.Database('data', df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Biogeme variables`**<br>\n",
    "Additionally, we need to make the variables in our data set globally available so we can use them in the estimation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biogeme variables\n",
    "for c in biodata.data.columns:\n",
    "    if biodata.data[c].dtypes != 'object': # This excludes the 'object' type columns\n",
    "        globals()[c] = biodata.variables[c]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3. The linear-additive RUM-MNL model` <br>\n",
    "In practice, choice modelling always starts with the estimation of simple models. The most common model to start with is the linear-additive RUM-MNL model. This assumes that the utility of each alternative is a linear function of the attributes of the alternatives. After having estimated this benchmark model, we can pursue more complex models, like Latent Class choice models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`The linear-additive utility specification`**\n",
    "\n",
    "We start with defining the utility specification of the model that we wish to estimate.<br>\n",
    "\n",
    "For that, we must define the parameters to be estimated and specify the utility functions.<br>\n",
    "\n",
    "In the linear-additive RUM-MNL model, the observed utility is *V* for alternative *i* is given by:\n",
    "\n",
    "$V_i = \\beta_1 \\cdot \\text{x}_{1i} + \\beta_2 \\cdot \\text{x}_{2i} + \\ldots + \\beta_M \\cdot \\text{x}_{Mi}  $\n",
    "\n",
    "Where:\n",
    "- $\\beta_1, \\beta_2, \\ldots, \\beta_M$ denote the marginal utility associated with each attribute $m$.\n",
    "- $\\text{x}_{1i}, \\text{x}_{2i}, \\ldots, \\text{x}_{Mi} $ correspond to the attribute values alternative *i*.\n",
    "\n",
    "The cell below creates this utility function in Biogeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the model    \n",
    "model_name = 'Linear-additive RUM-MNL'\n",
    "\n",
    "# Define the model parameters, using the function \"Beta()\", in which you must define:\n",
    "# the name of the parameter (string),\n",
    "# starting value (float), \n",
    "# lower bound (float),\n",
    "# upper bound (float), \n",
    "# 0 or 1, indicating if the parameter must be estimated. 0 means estimated, 1 means fixed to the starting value. \n",
    "B_tt    = Beta('B_tt',  0, None, None, 0)\n",
    "B_cong  = Beta('B_cong',0, None, None, 0)\n",
    "B_var   = Beta('B_var', 0, None, None, 0)\n",
    "B_tc    = Beta('B_tc',  0, None, None, 0)\n",
    "\n",
    "# Define the utility functions\n",
    "V1 = B_tt * TT1 + B_cong * CONG1 + B_var * VAR1 + B_tc * TC1\n",
    "V2 = B_tt * TT2 + B_cong * CONG2 + B_var * VAR2 + B_tc * TC2\n",
    "V3 = B_tt * TT3 + B_cong * CONG3 + B_var * VAR3 + B_tc * TC3\n",
    "\n",
    "# Associate utility functions with the numbering of alternatives in the \"choice\" column\n",
    "V = {1: V1, 2: V2, 3: V3}\n",
    "\n",
    "# Create a dictionary called av to describe the availability conditions of each alternative, where 1 indicates that the alternative is available, and 0 indicates that the alternative is not available.\n",
    "AV = {1: AV1, 2: AV2, 3: AV3} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Estimation function`** \n",
    "\n",
    "Now that we have specified the model, we need to estimate it. To do so, we use the following function `estimate_mnl()` that we import from `bio_estimation_fcns.py`. Open the file from the working folder, and inspect the code. <br>\n",
    "**Do you understand its structure?**.\n",
    "\n",
    "This function takes the following inputs:\n",
    "* V: a dictionary containing the utility functions for each alternative\n",
    "* AV: a dictionary containing availability conditions\n",
    "* CHOICE: an integer containing the choice variable\n",
    "* database: database object\n",
    "* model_name: name of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Estimation`**\n",
    "\n",
    "We have created a biogeme database (biodata) and we have defined our utility functions. Therefore, we can now estimate the model by bringing these ingredients together. That is, we pass the model specifications and the database to the estimation function. The function `estimate_mnl()` returns an object which contains the estimation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the model\n",
    "results_MNL = estimate_mnl(V, AV, CHOICE, biodata,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Estimation results`**\n",
    "\n",
    "Once we have estimated a mode, we want to see the estimation results. We can use the `print_results()` function to view the estimation statistics and the parameter estimates.\n",
    "\n",
    "**It reports the following estimation statistics:**\n",
    "* `Number of parameters`: Parameters being estimated.\n",
    "* `Sample size`: The number of observations in the data set (used for estimating the model).\n",
    "* `Excluded data`: The number of observations in the data set that were excluded for estimation.\n",
    "* `Null log-likelihood`: The log-likelihood of the null model.\n",
    "* `Final log-likelihood`: The log-likelihood of the estimated model.\n",
    "* `Likelihood ratio test (null)`: A statistical test comparing the null model's likelihood with the likelihood of the estimated model. \n",
    "* `Rho square (null)`: Quantifies how well the model explains the data compared to the null model.\n",
    "* `Rho bar square (null)`: Quantifies how well the model explains the data compared to the null model while penalising for the number of model parameters.\n",
    "* `Akaike Information Criterion (AIC)`: A measure that shows the goodness of fit of the model, where lower AIC values indicate better models.\n",
    "* `Bayesian Information Criterion (BIC)`: Similar to AIC, it penalizes model complexity more heavily, with lower values indicating better-fitting models while considering complexity.\n",
    "\n",
    "**It reports the following information about the parameter estimates:**\n",
    "\n",
    "* `Value`: This is the maximum likelihood estimate for the parameter\n",
    "* `Rob. Std Err`: The (robust) standard error associated with the maximum likelihood estimate\n",
    "* `Rob. t-test`: The t-value showing the significance, demonstrating the generalisability of the relationship to the population.\n",
    "* `Rob. p-value`: The p-value associated with the t-value showing the significance, demonstrating the generalisability of the relationship to the population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(results_MNL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the estimation results**\n",
    "\n",
    "        --> The estimation results show that the model has a good fit to the data. The estimated parameters all have the intuitively estimated signs, and are all significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `4. 2-class Latent class choice model` <br>\n",
    "\n",
    "Now, we have estimated the benchmark linear-additive MNL models, we can proceed with estimating more complex models. Next, we will estimate a 2-class LC choice model. We assume the two classes have different preferences for the attributes of the alternatives, but the utility functions within each class are still linear-additive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Database for panel models`** \n",
    "\n",
    "First, we create a new biogeme database. This is needed because now we take into account the panel structure of the data. By creating a new copy we ensure that both MNL and LC models are estimated in the correct way.\n",
    "Using the `.panel()` function, we can specify the panel structure of the data. In this case, we specify that the panel structure is defined by the `ID` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "biodata_panel = db.Database('data_panel', df) # Creates a biogeme database\n",
    "biodata_panel.panel(\"ID\")                     # Specify that the panel structure is defined by \"ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Parameters and utility function definition`**\n",
    "\n",
    "Next, we define the parameters and utility functions for the 2-class LC model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the model    \n",
    "model_name = 'LC with 2 classes'\n",
    "\n",
    "# Define the model parameters for class 0\n",
    "B_tt_0    = Beta('B_tt_0',  0, None, None, 0)\n",
    "B_cong_0  = Beta('B_cong_0',0, None, None, 0)\n",
    "B_var_0   = Beta('B_var_0', 0, None, None, 0)\n",
    "B_tc_0    = Beta('B_tc_0',  0, None, None, 0)\n",
    "\n",
    "# Define the utility functions for class 1\n",
    "B_tt_1    = Beta('B_tt_1',  0, None, None, 0)\n",
    "B_cong_1  = Beta('B_cong_1',0, None, None, 0)\n",
    "B_var_1   = Beta('B_var_1', 0, None, None, 0)\n",
    "B_tc_1    = Beta('B_tc_1',  0, None, None, 0)\n",
    "\n",
    "# Define the membership model parameters\n",
    "delta_0 = Beta('delta_0',   0,    None, None, 1)\n",
    "delta_1 = Beta('delta_1',   0.10, None, None, 0)\n",
    "\n",
    "# Define the utility functions for class 0\n",
    "V1_0 = B_tt_0 * TT1 + B_cong_0 * CONG1 + B_var_0 * VAR1 + B_tc_0 * TC1\n",
    "V2_0 = B_tt_0 * TT2 + B_cong_0 * CONG2 + B_var_0 * VAR2 + B_tc_0 * TC2\n",
    "V3_0 = B_tt_0 * TT3 + B_cong_0 * CONG3 + B_var_0 * VAR3 + B_tc_0 * TC3\n",
    "\n",
    "# Define the utility functions for class 1\n",
    "V1_1 = B_tt_1 * TT1 + B_cong_1 * CONG1 + B_var_1 * VAR1 + B_tc_1 * TC1\n",
    "V2_1 = B_tt_1 * TT2 + B_cong_1 * CONG2 + B_var_1 * VAR2 + B_tc_1 * TC2\n",
    "V3_1 = B_tt_1 * TT3 + B_cong_1 * CONG3 + B_var_1 * VAR3 + B_tc_1 * TC3\n",
    "\n",
    "# Create a dictionary associating utility functions with the numbering of alternatives in the \"choice\" column\n",
    "V_0 = {1: V1_0, 2: V2_0, 3: V3_0}\n",
    "V_1 = {1: V1_1, 2: V2_1, 3: V3_1}\n",
    "\n",
    "# Put the dictionaries of utility functions in a list\n",
    "V = [V_0, V_1]\n",
    "\n",
    "# Create a dictionary to describe the availability conditions of each alternative, where 1 indicates that the alternative is available, and 0 indicates that the alternative is not available.\n",
    "AV = {1: AV1, 2: AV2, 3: AV3} \n",
    "\n",
    "# Define the membership model value functions for each class\n",
    "nu_0 = delta_0  # Note: one class needs to be fixed to 0. delta_0 is fixed to 0\n",
    "nu_1 = delta_1  # In this case, we only estimate a class membership constant delta_1\n",
    "\n",
    "# Put membership functions in a list\n",
    "nu = [nu_0, nu_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Estimation`**\n",
    "\n",
    "We have created a biogeme pane; database (biodata_pane;) and we have defined our utility functions for the LC model. Therefore, we can now estimate the LC model by bringing these ingredients together. That is, we pass the model specifications and the database to the estimation function. The function `estimate_LC()` returns an object which contains the estimation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the LC model\n",
    "results_LC = estimate_LC(V,AV,nu,CHOICE, biodata_panel,model_name)\n",
    "\n",
    "# Print the results\n",
    "print_results(results_LC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 1:  Interpretation of the LC model with 2 classes `<br>\n",
    "\n",
    "`A.` Did the model fit (LL) improve as compared to the MNL model?<br>\n",
    "`B.` Are the estimates all statistically significant?<br>\n",
    "`C.` Carefully inspect the parameter estimates. What can you say about the differences in the tastes across the two classes?<br>\n",
    "`D.` Which class is the largest? What does that mean? <br>\n",
    "`E.` Calculate the \"market shares\" for each class using the estimated deltas. Hint, you can do this with a calculator, or using numpy. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 2:  Estimate an LC model with 3 classes `<br>\n",
    "To do so, you need to:\n",
    "* Create a new cell and copy the code from the 2-class LC model\n",
    "* Extend the code to three classes\n",
    "* Estimate the model\n",
    "* Print the results\n",
    "\n",
    "`A.` Did the model fit (LL) improve as compared to the 2-class model? <br>\n",
    "`B.` One of the class constants (deltas) is probably insignificant. What does this tell us? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEN1721",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
